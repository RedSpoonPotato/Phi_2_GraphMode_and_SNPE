{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 18:47:44.803699: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 18:47:45.388062: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 18:47:45.391629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 18:47:46.782790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 18:47:49.696862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-02 18:47:49.698501: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layernorm_weight :  (2560,)\n",
      "layernorm_bias :  (2560,)\n",
      "q_proj_weight :  (2560, 2560)\n",
      "q_proj_bias :  (2560,)\n",
      "k_proj_weight :  (2560, 2560)\n",
      "k_proj_bias :  (2560,)\n",
      "v_proj_weight :  (2560, 2560)\n",
      "v_proj_bias :  (2560,)\n",
      "dense_weight :  (2560, 2560)\n",
      "dense_bias :  (2560,)\n",
      "mlp_fc1_weight :  (2560, 10240)\n",
      "mlp_fc1_bias :  (10240,)\n",
      "mlp_fc2_weight :  (10240, 2560)\n",
      "mlp_fc2_bias :  (2560,)\n",
      "layernorm_weight :  (2560,)\n",
      "layernorm_bias :  (2560,)\n",
      "q_proj_weight :  (2560, 2560)\n",
      "q_proj_bias :  (2560,)\n",
      "k_proj_weight :  (2560, 2560)\n",
      "k_proj_bias :  (2560,)\n",
      "v_proj_weight :  (2560, 2560)\n",
      "v_proj_bias :  (2560,)\n",
      "dense_weight :  (2560, 2560)\n",
      "dense_bias :  (2560,)\n",
      "mlp_fc1_weight :  (2560, 10240)\n",
      "mlp_fc1_bias :  (10240,)\n",
      "mlp_fc2_weight :  (10240, 2560)\n",
      "mlp_fc2_bias :  (2560,)\n",
      "layernorm_weight :  (2560,)\n",
      "layernorm_bias :  (2560,)\n",
      "q_proj_weight :  (2560, 2560)\n",
      "q_proj_bias :  (2560,)\n",
      "k_proj_weight :  (2560, 2560)\n",
      "k_proj_bias :  (2560,)\n",
      "v_proj_weight :  (2560, 2560)\n",
      "v_proj_bias :  (2560,)\n",
      "dense_weight :  (2560, 2560)\n",
      "dense_bias :  (2560,)\n",
      "mlp_fc1_weight :  (2560, 10240)\n",
      "mlp_fc1_bias :  (10240,)\n",
      "mlp_fc2_weight :  (10240, 2560)\n",
      "mlp_fc2_bias :  (2560,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dtype = tf.float16 # subject to change\n",
    "BASE_DIR = \"/home/kernal1/QM_Sandbox/Phi_2/phi_local/weights/\"\n",
    "# DECODER_LAYERS = 3 # PUT BACK IN\n",
    "DECODER_LAYERS = 3  # REMOVE\n",
    "params = {}\n",
    "params['embed_tokens'] = tf.constant(np.load(BASE_DIR + 'embed_tokens.npy'), dtype=dtype)\n",
    "params['decoder_layers'] = []\n",
    "\n",
    "for i in range(DECODER_LAYERS):\n",
    "  layer_params = {}\n",
    "  layer_params['layernorm_weight'] = 'layernorm_weight.npy'\n",
    "  layer_params['layernorm_bias'] = 'layernorm_bias'\n",
    "  layer_params['q_proj_weight'] = 'q_proj_weight'\n",
    "  layer_params['q_proj_bias'] = 'q_proj_bias'\n",
    "  layer_params['k_proj_weight'] = 'k_proj_weight'\n",
    "  layer_params['k_proj_bias'] = 'k_proj_bias'\n",
    "  layer_params['v_proj_weight'] = 'v_proj_weight'\n",
    "  layer_params['v_proj_bias'] = 'v_proj_bias'\n",
    "  layer_params['dense_weight'] = 'dense_weight'\n",
    "  layer_params['dense_bias'] = 'dense_bias'\n",
    "  layer_params['mlp_fc1_weight'] = 'mlp_fc1_weight'\n",
    "  layer_params['mlp_fc1_bias'] = 'mlp_fc1_bias'\n",
    "  layer_params['mlp_fc2_weight'] = 'mlp_fc2_weight'\n",
    "  layer_params['mlp_fc2_bias'] = 'mlp_fc2_bias'\n",
    "  for key in layer_params:\n",
    "    layer_params[key] = tf.constant(np.load(BASE_DIR + str(i) + '_' + key + '.npy'), dtype=dtype)\n",
    "    if \"weight\" in key and \"layernorm\" not in key:\n",
    "      layer_params[key] = tf.transpose(layer_params[key], perm=[1,0])\n",
    "    print(key, \": \", layer_params[key].shape) # remove later\n",
    "  params['decoder_layers'].append(layer_params)\n",
    "\n",
    "params['final_layernorm_weight'] = tf.constant(np.load(BASE_DIR + 'final_layernorm_weight.npy'), dtype=dtype)\n",
    "params['final_layernorm_bias'] = tf.constant(np.load(BASE_DIR + 'final_layernorm_bias.npy'), dtype=dtype)\n",
    "\n",
    "params['lm_head_weight'] = tf.transpose(tf.constant(np.load(BASE_DIR + 'lm_head_weight.npy'), dtype=dtype), perm=[1,0])\n",
    "params['lm_head_bias'] = tf.constant(np.load(BASE_DIR + 'lm_head_bias.npy'), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# params['embed_tokens'].numpy().tofile('embed_tokens.dat')\n",
    "\n",
    "# x = None\n",
    "# y = None\n",
    "\n",
    "# # tf.reshape(params['final_layernorm_weight'], [2560])\n",
    "# # params['lm_head_weight']\n",
    "# # params['lm_head_bias']\n",
    "# y = tf.concat([\n",
    "#     tf.reshape(params['final_layernorm_weight'], [2560]),\n",
    "#     tf.reshape(params['final_layernorm_bias'], [2560]),\n",
    "#     tf.reshape(params['lm_head_weight'], [2560*51200]),\n",
    "#     tf.reshape(params['lm_head_bias'], [51200])\n",
    "# ], axis=-1)\n",
    "\n",
    "# y.numpy().tofile('lm_head_weights_1.dat')\n",
    "\n",
    "# layer_params = params['decoder_layers']\n",
    "\n",
    "\n",
    "# x = tf.concat([\n",
    "#         tf.reshape(layer_params[0]['layernorm_weight'], [2560]),\n",
    "#         tf.reshape(layer_params[0]['layernorm_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[0]['q_proj_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[0]['q_proj_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[0]['k_proj_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[0]['k_proj_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[0]['v_proj_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[0]['v_proj_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[0]['dense_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[0]['dense_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[0]['mlp_fc1_weight'], [2560 * 10240]),\n",
    "#         tf.reshape(layer_params[0]['mlp_fc1_bias'], [10240]),\n",
    "#         tf.reshape(layer_params[0]['mlp_fc2_weight'], [10240 * 2560]),\n",
    "#         tf.reshape(layer_params[0]['mlp_fc2_bias'], [2560]),\n",
    "#         ], axis=-1)\n",
    "\n",
    "# for i in range(1, DECODER_LAYERS):\n",
    "#     print(i)\n",
    "#     x = tf.concat([x, \n",
    "#         tf.reshape(layer_params[i]['layernorm_weight'], [2560]),\n",
    "#         tf.reshape(layer_params[i]['layernorm_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[i]['q_proj_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[i]['q_proj_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[i]['k_proj_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[i]['k_proj_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[i]['v_proj_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[i]['v_proj_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[i]['dense_weight'], [2560 * 2560]),\n",
    "#         tf.reshape(layer_params[i]['dense_bias'], [2560]),\n",
    "#         tf.reshape(layer_params[i]['mlp_fc1_weight'], [2560 * 10240]),\n",
    "#         tf.reshape(layer_params[i]['mlp_fc1_bias'], [10240]),\n",
    "#         tf.reshape(layer_params[i]['mlp_fc2_weight'], [10240 * 2560]),\n",
    "#         tf.reshape(layer_params[i]['mlp_fc2_bias'], [2560]),\n",
    "#         ], axis=-1)\n",
    "\n",
    "\n",
    "# x.numpy().tofile('decoder_weights_1.dat') # careful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiConfig():\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=51200,\n",
    "        hidden_size=2560,\n",
    "        intermediate_size=10240,\n",
    "        num_hidden_layers=DECODER_LAYERS, # modified,\n",
    "        num_attention_heads=32,\n",
    "        num_key_value_heads=32,\n",
    "        resid_pdrop=0.0, # the model bt default has 0.1 on colab, but we dont have randomness\n",
    "        embd_pdrop=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        hidden_act=\"gelu_new\",\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.00,\n",
    "        layer_norm_eps=1e-5,\n",
    "        use_cache=True,                  # Modifed\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        partial_rotary_factor=0.4,\n",
    "        qk_layernorm=False,\n",
    "        bos_token_id=50256,\n",
    "        eos_token_id=50256,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        if num_key_value_heads is None:\n",
    "            num_key_value_heads = num_attention_heads\n",
    "\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.hidden_act = hidden_act\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.partial_rotary_factor = partial_rotary_factor\n",
    "        self.qk_layernorm = qk_layernorm\n",
    "        self.pad_token_id = None\n",
    "        self._attn_implementation = 'eager'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_phi_gm\n",
    "config_gm = PhiConfig()\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(my_phi_gm)\n",
    "importlib.reload(my_phi_gm.bert)\n",
    "importlib.reload(my_phi_gm.masking_utils_gm)\n",
    "importlib.reload(my_phi_gm.cache)\n",
    "config_gm = PhiConfig(use_cache=True)\n",
    "my_model_gm = None\n",
    "my_model_gm = my_phi_gm.PhiForCausalLM(config_gm, params)\n",
    "# testing input to see if graphmode works\n",
    "input_ids_gm = tf.constant([[40, 588, 836, 5500, 13, 1867, 318, 534, 4004, 2057, 30]])\n",
    "mask_gm = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "model_kwargs_gm = {'use_cache': True, 'attention_mask': mask_gm}\n",
    "model_inputs_gm = my_model_gm.prepare_inputs_for_generation(input_ids_gm, **model_kwargs_gm)\n",
    "out_gm = my_model_gm(\n",
    "        **model_inputs_gm,\n",
    "        return_dict=False, # important\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "        )\n",
    "print(out_gm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop version\n",
    "importlib.reload(my_phi_gm)\n",
    "importlib.reload(my_phi_gm.bert)\n",
    "importlib.reload(my_phi_gm.masking_utils_gm)\n",
    "importlib.reload(my_phi_gm.cache)\n",
    "my_model_gm = None\n",
    "# config = PhiConfig(num_hidden_layers=3) # old line\n",
    "config_gm = PhiConfig(use_cache=True)\n",
    "my_model_gm = my_phi_gm.PhiForCausalLM(config_gm, params)\n",
    "\n",
    "input_ids_gm = tf.constant([[40, 588, 836, 5500, 13, 1867, 318, 534, 4004, 2057, 30]])\n",
    "mask_gm = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "model_kwargs_gm = {'use_cache': True, 'attention_mask': mask_gm}\n",
    "\n",
    "# model_inputs(keys) after first call in orginal code: dict_keys(['input_ids', 'position_ids', 'past_key_values', 'use_cache', 'attention_mask'])\n",
    "\n",
    "# prepare_inputs_for_generation on first run looks good (not sure about 2nd)\n",
    "\n",
    "for i in range(2):\n",
    "  # print(\"model_kwargs:\", model_kwargs_gm)\n",
    "  model_inputs_gm = my_model_gm.prepare_inputs_for_generation(input_ids_gm, **model_kwargs_gm)\n",
    "  # print(\"model_inputs\", model_inputs_gm)\n",
    "  # print(model_inputs.keys())\n",
    "  print(\"model_inputs[attention_mask]\", model_inputs_gm[\"attention_mask\"])\n",
    "  out = my_model_gm(\n",
    "          **model_inputs_gm,\n",
    "          return_dict=False, # important\n",
    "          output_attentions=False,\n",
    "          output_hidden_states=False\n",
    "         )\n",
    "  print(\"out length:\", len(out))\n",
    "\n",
    "  print(\"logits.shape\", out[0].shape)\n",
    "  # print(\"out.past_key_values[0][0].shape:\", out.past_key_values[0][0].shape)\n",
    "  next_token_logits = out[0][:, -1, :]\n",
    "  next_tokens = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "  input_ids_gm = tf.concat([input_ids_gm, next_tokens[:, None]], axis=-1)\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  # need to update past_key_values as apart of model_kwargs\n",
    "  # need to update size of mask (looks like we did, not sure about proper values)\n",
    "  # past_key_value shape[0][0]: ([1, 32, 11, 80]) after first run of model in orginal code\n",
    "  attention_mask = model_kwargs_gm[\"attention_mask\"]\n",
    "  model_kwargs_gm[\"attention_mask\"] = tf.concat([attention_mask, \n",
    "                                                 tf.ones((attention_mask.shape[0], 1), \n",
    "                                                         dtype=attention_mask.dtype)], \n",
    "                                                         axis=-1)\n",
    "  print(\"model_kwargs[attention_mask].shape\", model_kwargs_gm[\"attention_mask\"].shape)\n",
    "  model_kwargs_gm[\"past_key_values\"] = out[1]\n",
    "  # ending stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_inputs_gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 2048 10000.0\n",
      "initial sin_cos size: (2048, 32)\n",
      "32 2048 10000.0\n",
      "initial sin_cos size: (2048, 32)\n",
      "32 2048 10000.0\n",
      "initial sin_cos size: (2048, 32)\n",
      "---------\n",
      "MODEL_INPUTS:\n",
      " MASK: tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 11), dtype=int32)\n",
      "POS_IDS: tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32) \n",
      "\n",
      "---------\n",
      "\n",
      "model_inputs[attention_mask] tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 11), dtype=int32)\n",
      "input_ids.dtype <dtype: 'int32'>\n",
      "inputs_embeds.dtype <dtype: 'float16'>\n",
      "after embedding: hidden states shape: (1, 11, 2560)\n",
      "_______TESTING SIZES____\n",
      "(1, 11, 2560)\n",
      "(1, 1, 11, 11)\n",
      "(1, 11)\n",
      "position ids:  tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "residual: (1, 11, 2560)\n",
      "hidden_states.shape:  (1, 11, 2560)\n",
      "attention_mask.shape:  (1, 1, 11, 11)\n",
      "position_ids.shape: (1, 11)\n",
      "attention_mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "query_states.shape after proj: (1, 11, 2560)\n",
      "query_states after transpose (1, 32, 11, 80)\n",
      "key_states after transpose (1, 32, 11, 80)\n",
      "\tcos_cached_size: (2048, 32)\n",
      "value-states shape: (1, 32, 11, 80)\n",
      "cos shape before: (11, 32)\n",
      "gather of cos shape: (1, 11, 32)\n",
      "q shape:  (1, 32, 11, 32)\n",
      "cos shape:  (1, 1, 11, 32)\n",
      "k shape:  (1, 32, 11, 32)\n",
      "sin shape:  (1, 1, 11, 32)\n",
      "(q * cos).shape (1, 32, 11, 32)\n",
      "(rotate_half(q) * sin).shape (1, 32, 11, 32)\n",
      "key_rot after apply_rotary_pos_emb (1, 32, 11, 32)\n",
      "key_states after concatenation (1, 32, 11, 80)\n",
      "layer_id:  0\n",
      "done updating\n",
      "query_states.shape on P2 (1, 32, 11, 80)\n",
      "key_states.shape on P2 (1, 32, 11, 80)\n",
      "value_states.shape on P2 (1, 32, 11, 80)\n",
      "attn_weights after matmul but before sqrt (1, 32, 11, 11)\n",
      "attn-weight after first matmul and sqrt: (1, 32, 11, 11)\n",
      "attn_weights.shape right before applying mask: (1, 32, 11, 11)\n",
      "attention_mask before applying mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "attn_weights after applying mask (1, 32, 11, 11)\n",
      "attn_weights.shape after softmaxing (1, 32, 11, 11)\n",
      "attn_output.shape after matmul (1, 32, 11, 80)\n",
      "attn_output after tranpose and reshaping (1, 11, 2560)\n",
      "attn_output after dense layer: (1, 11, 2560)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Dense_v2.__call__ at 0x7fbf6942f790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Dense_v2.__call__ at 0x7fbf6942fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "feed_forward_hidden_states.shape (1, 11, 2560)\n",
      "hidden_states after sum in decoder: (1, 11, 2560)\n",
      "_______TESTING SIZES____\n",
      "(1, 11, 2560)\n",
      "(1, 1, 11, 11)\n",
      "(1, 11)\n",
      "position ids:  tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "residual: (1, 11, 2560)\n",
      "hidden_states.shape:  (1, 11, 2560)\n",
      "attention_mask.shape:  (1, 1, 11, 11)\n",
      "position_ids.shape: (1, 11)\n",
      "attention_mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "query_states.shape after proj: (1, 11, 2560)\n",
      "query_states after transpose (1, 32, 11, 80)\n",
      "key_states after transpose (1, 32, 11, 80)\n",
      "\tcos_cached_size: (2048, 32)\n",
      "value-states shape: (1, 32, 11, 80)\n",
      "cos shape before: (11, 32)\n",
      "gather of cos shape: (1, 11, 32)\n",
      "q shape:  (1, 32, 11, 32)\n",
      "cos shape:  (1, 1, 11, 32)\n",
      "k shape:  (1, 32, 11, 32)\n",
      "sin shape:  (1, 1, 11, 32)\n",
      "(q * cos).shape (1, 32, 11, 32)\n",
      "(rotate_half(q) * sin).shape (1, 32, 11, 32)\n",
      "key_rot after apply_rotary_pos_emb (1, 32, 11, 32)\n",
      "key_states after concatenation (1, 32, 11, 80)\n",
      "layer_id:  1\n",
      "done updating\n",
      "query_states.shape on P2 (1, 32, 11, 80)\n",
      "key_states.shape on P2 (1, 32, 11, 80)\n",
      "value_states.shape on P2 (1, 32, 11, 80)\n",
      "attn_weights after matmul but before sqrt (1, 32, 11, 11)\n",
      "attn-weight after first matmul and sqrt: (1, 32, 11, 11)\n",
      "attn_weights.shape right before applying mask: (1, 32, 11, 11)\n",
      "attention_mask before applying mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "attn_weights after applying mask (1, 32, 11, 11)\n",
      "attn_weights.shape after softmaxing (1, 32, 11, 11)\n",
      "attn_output.shape after matmul (1, 32, 11, 80)\n",
      "attn_output after tranpose and reshaping (1, 11, 2560)\n",
      "attn_output after dense layer: (1, 11, 2560)\n",
      "feed_forward_hidden_states.shape (1, 11, 2560)\n",
      "hidden_states after sum in decoder: (1, 11, 2560)\n",
      "_______TESTING SIZES____\n",
      "(1, 11, 2560)\n",
      "(1, 1, 11, 11)\n",
      "(1, 11)\n",
      "position ids:  tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "residual: (1, 11, 2560)\n",
      "hidden_states.shape:  (1, 11, 2560)\n",
      "attention_mask.shape:  (1, 1, 11, 11)\n",
      "position_ids.shape: (1, 11)\n",
      "attention_mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "query_states.shape after proj: (1, 11, 2560)\n",
      "query_states after transpose (1, 32, 11, 80)\n",
      "key_states after transpose (1, 32, 11, 80)\n",
      "\tcos_cached_size: (2048, 32)\n",
      "value-states shape: (1, 32, 11, 80)\n",
      "cos shape before: (11, 32)\n",
      "gather of cos shape: (1, 11, 32)\n",
      "q shape:  (1, 32, 11, 32)\n",
      "cos shape:  (1, 1, 11, 32)\n",
      "k shape:  (1, 32, 11, 32)\n",
      "sin shape:  (1, 1, 11, 32)\n",
      "(q * cos).shape (1, 32, 11, 32)\n",
      "(rotate_half(q) * sin).shape (1, 32, 11, 32)\n",
      "key_rot after apply_rotary_pos_emb (1, 32, 11, 32)\n",
      "key_states after concatenation (1, 32, 11, 80)\n",
      "layer_id:  2\n",
      "done updating\n",
      "query_states.shape on P2 (1, 32, 11, 80)\n",
      "key_states.shape on P2 (1, 32, 11, 80)\n",
      "value_states.shape on P2 (1, 32, 11, 80)\n",
      "attn_weights after matmul but before sqrt (1, 32, 11, 11)\n",
      "attn-weight after first matmul and sqrt: (1, 32, 11, 11)\n",
      "attn_weights.shape right before applying mask: (1, 32, 11, 11)\n",
      "attention_mask before applying mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "attn_weights after applying mask (1, 32, 11, 11)\n",
      "attn_weights.shape after softmaxing (1, 32, 11, 11)\n",
      "attn_output.shape after matmul (1, 32, 11, 80)\n",
      "attn_output after tranpose and reshaping (1, 11, 2560)\n",
      "attn_output after dense layer: (1, 11, 2560)\n",
      "feed_forward_hidden_states.shape (1, 11, 2560)\n",
      "hidden_states after sum in decoder: (1, 11, 2560)\n",
      "out length: 2\n",
      "logits.shape (1, 11, 51200)\n",
      "----out[0].shape:  (1, 11, 51200)\n",
      "next token predicted:  tf.Tensor([220], shape=(1,), dtype=int32)\n",
      "\n",
      "\n",
      "\n",
      "model_kwargs[attention_mask].shape (1, 12)\n",
      "cache length: 11\n",
      "---------\n",
      "MODEL_INPUTS:\n",
      " MASK: tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 12), dtype=int32)\n",
      "POS_IDS: tf.Tensor([[11]], shape=(1, 1), dtype=int32) \n",
      "\n",
      "---------\n",
      "\n",
      "model_inputs[attention_mask] tf.Tensor([[1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 12), dtype=int32)\n",
      "input_ids.dtype <dtype: 'int32'>\n",
      "inputs_embeds.dtype <dtype: 'float16'>\n",
      "after embedding: hidden states shape: (1, 1, 2560)\n",
      "_______TESTING SIZES____\n",
      "(1, 1, 2560)\n",
      "(1, 1, 1, 12)\n",
      "(1, 1)\n",
      "position ids:  tf.Tensor([[11]], shape=(1, 1), dtype=int32)\n",
      "residual: (1, 1, 2560)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function LayerNorm.__call__ at 0x7fbf7c1609d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "hidden_states.shape:  (1, 1, 2560)\n",
      "attention_mask.shape:  (1, 1, 1, 12)\n",
      "position_ids.shape: (1, 1)\n",
      "attention_mask: tf.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 12), dtype=float16)\n",
      "query_states.shape after proj: (1, 1, 2560)\n",
      "query_states after transpose (1, 32, 1, 80)\n",
      "key_states after transpose (1, 32, 1, 80)\n",
      "\tcos_cached_size: (2048, 32)\n",
      "value-states shape: (1, 32, 1, 80)\n",
      "cos shape before: (12, 32)\n",
      "gather of cos shape: (1, 1, 32)\n",
      "q shape:  (1, 32, 1, 32)\n",
      "cos shape:  (1, 1, 1, 32)\n",
      "k shape:  (1, 32, 1, 32)\n",
      "sin shape:  (1, 1, 1, 32)\n",
      "(q * cos).shape (1, 32, 1, 32)\n",
      "(rotate_half(q) * sin).shape (1, 32, 1, 32)\n",
      "key_rot after apply_rotary_pos_emb (1, 32, 1, 32)\n",
      "key_states after concatenation (1, 32, 1, 80)\n",
      "layer_id:  0\n",
      "done updating\n",
      "query_states.shape on P2 (1, 32, 1, 80)\n",
      "key_states.shape on P2 (1, 32, 12, 80)\n",
      "value_states.shape on P2 (1, 32, 12, 80)\n",
      "attn_weights after matmul but before sqrt (1, 32, 1, 12)\n",
      "attn-weight after first matmul and sqrt: (1, 32, 1, 12)\n",
      "attn_weights.shape right before applying mask: (1, 32, 1, 12)\n",
      "attention_mask before applying mask: tf.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 12), dtype=float16)\n",
      "attn_weights after applying mask (1, 32, 1, 12)\n",
      "attn_weights.shape after softmaxing (1, 32, 1, 12)\n",
      "attn_output.shape after matmul (1, 32, 1, 80)\n",
      "attn_output after tranpose and reshaping (1, 1, 2560)\n",
      "attn_output after dense layer: (1, 1, 2560)\n",
      "feed_forward_hidden_states.shape (1, 1, 2560)\n",
      "hidden_states after sum in decoder: (1, 1, 2560)\n",
      "_______TESTING SIZES____\n",
      "(1, 1, 2560)\n",
      "(1, 1, 1, 12)\n",
      "(1, 1)\n",
      "position ids:  tf.Tensor([[11]], shape=(1, 1), dtype=int32)\n",
      "residual: (1, 1, 2560)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function LayerNorm.__call__ at 0x7fbf692c99d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "hidden_states.shape:  (1, 1, 2560)\n",
      "attention_mask.shape:  (1, 1, 1, 12)\n",
      "position_ids.shape: (1, 1)\n",
      "attention_mask: tf.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 12), dtype=float16)\n",
      "query_states.shape after proj: (1, 1, 2560)\n",
      "query_states after transpose (1, 32, 1, 80)\n",
      "key_states after transpose (1, 32, 1, 80)\n",
      "\tcos_cached_size: (2048, 32)\n",
      "value-states shape: (1, 32, 1, 80)\n",
      "cos shape before: (12, 32)\n",
      "gather of cos shape: (1, 1, 32)\n",
      "q shape:  (1, 32, 1, 32)\n",
      "cos shape:  (1, 1, 1, 32)\n",
      "k shape:  (1, 32, 1, 32)\n",
      "sin shape:  (1, 1, 1, 32)\n",
      "(q * cos).shape (1, 32, 1, 32)\n",
      "(rotate_half(q) * sin).shape (1, 32, 1, 32)\n",
      "key_rot after apply_rotary_pos_emb (1, 32, 1, 32)\n",
      "key_states after concatenation (1, 32, 1, 80)\n",
      "layer_id:  1\n",
      "done updating\n",
      "query_states.shape on P2 (1, 32, 1, 80)\n",
      "key_states.shape on P2 (1, 32, 12, 80)\n",
      "value_states.shape on P2 (1, 32, 12, 80)\n",
      "attn_weights after matmul but before sqrt (1, 32, 1, 12)\n",
      "attn-weight after first matmul and sqrt: (1, 32, 1, 12)\n",
      "attn_weights.shape right before applying mask: (1, 32, 1, 12)\n",
      "attention_mask before applying mask: tf.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 12), dtype=float16)\n",
      "attn_weights after applying mask (1, 32, 1, 12)\n",
      "attn_weights.shape after softmaxing (1, 32, 1, 12)\n",
      "attn_output.shape after matmul (1, 32, 1, 80)\n",
      "attn_output after tranpose and reshaping (1, 1, 2560)\n",
      "attn_output after dense layer: (1, 1, 2560)\n",
      "feed_forward_hidden_states.shape (1, 1, 2560)\n",
      "hidden_states after sum in decoder: (1, 1, 2560)\n",
      "_______TESTING SIZES____\n",
      "(1, 1, 2560)\n",
      "(1, 1, 1, 12)\n",
      "(1, 1)\n",
      "position ids:  tf.Tensor([[11]], shape=(1, 1), dtype=int32)\n",
      "residual: (1, 1, 2560)\n",
      "hidden_states.shape:  (1, 1, 2560)\n",
      "attention_mask.shape:  (1, 1, 1, 12)\n",
      "position_ids.shape: (1, 1)\n",
      "attention_mask: tf.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 12), dtype=float16)\n",
      "query_states.shape after proj: (1, 1, 2560)\n",
      "query_states after transpose (1, 32, 1, 80)\n",
      "key_states after transpose (1, 32, 1, 80)\n",
      "\tcos_cached_size: (2048, 32)\n",
      "value-states shape: (1, 32, 1, 80)\n",
      "cos shape before: (12, 32)\n",
      "gather of cos shape: (1, 1, 32)\n",
      "q shape:  (1, 32, 1, 32)\n",
      "cos shape:  (1, 1, 1, 32)\n",
      "k shape:  (1, 32, 1, 32)\n",
      "sin shape:  (1, 1, 1, 32)\n",
      "(q * cos).shape (1, 32, 1, 32)\n",
      "(rotate_half(q) * sin).shape (1, 32, 1, 32)\n",
      "key_rot after apply_rotary_pos_emb (1, 32, 1, 32)\n",
      "key_states after concatenation (1, 32, 1, 80)\n",
      "layer_id:  2\n",
      "done updating\n",
      "query_states.shape on P2 (1, 32, 1, 80)\n",
      "key_states.shape on P2 (1, 32, 12, 80)\n",
      "value_states.shape on P2 (1, 32, 12, 80)\n",
      "attn_weights after matmul but before sqrt (1, 32, 1, 12)\n",
      "attn-weight after first matmul and sqrt: (1, 32, 1, 12)\n",
      "attn_weights.shape right before applying mask: (1, 32, 1, 12)\n",
      "attention_mask before applying mask: tf.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 12), dtype=float16)\n",
      "attn_weights after applying mask (1, 32, 1, 12)\n",
      "attn_weights.shape after softmaxing (1, 32, 1, 12)\n",
      "attn_output.shape after matmul (1, 32, 1, 80)\n",
      "attn_output after tranpose and reshaping (1, 1, 2560)\n",
      "attn_output after dense layer: (1, 1, 2560)\n",
      "feed_forward_hidden_states.shape (1, 1, 2560)\n",
      "hidden_states after sum in decoder: (1, 1, 2560)\n",
      "out length: 2\n",
      "logits.shape (1, 1, 51200)\n",
      "----out[0].shape:  (1, 1, 51200)\n",
      "next token predicted:  tf.Tensor([628], shape=(1,), dtype=int32)\n",
      "\n",
      "\n",
      "\n",
      "model_kwargs[attention_mask].shape (1, 13)\n"
     ]
    }
   ],
   "source": [
    "# eager mode loop\n",
    "# loop version\n",
    "importlib.reload(my_phi)\n",
    "importlib.reload(my_phi.bert)\n",
    "importlib.reload(my_phi.masking_utils_gm)\n",
    "importlib.reload(my_phi.cache)\n",
    "my_model = None\n",
    "# config = PhiConfig(num_hidden_layers=3) # old line\n",
    "config = PhiConfig(use_cache=True)\n",
    "my_model = my_phi.PhiForCausalLM(config, params)\n",
    "\n",
    "input_ids = tf.constant([[2061, 318, 534, 4004, 3124, 30, 13, 11517, 318, 2266, 13]])\n",
    "mask = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "model_kwargs = {'use_cache': True, 'attention_mask': mask}\n",
    "\n",
    "# model_inputs(keys) after first call in orginal code: dict_keys(['input_ids', 'position_ids', 'past_key_values', 'use_cache', 'attention_mask'])\n",
    "\n",
    "# prepare_inputs_for_generation on first run looks good (not sure about 2nd)\n",
    "\n",
    "for i in range(2):\n",
    "  # print(\"model_kwargs:\", model_kwargs)\n",
    "  model_inputs = my_model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "  print(\"---------\\nMODEL_INPUTS:\\n\", \"MASK:\" , model_inputs['attention_mask'])\n",
    "  print(\"POS_IDS:\", model_inputs['position_ids'], \"\\n\\n---------\\n\")\n",
    "  # print(\"model_inputs\", model_inputs)\n",
    "  # print(model_inputs.keys())\n",
    "  print(\"model_inputs[attention_mask]\", model_inputs[\"attention_mask\"])\n",
    "  out = my_model(\n",
    "          **model_inputs,\n",
    "          return_dict=False, # important\n",
    "          output_attentions=False,\n",
    "          output_hidden_states=False\n",
    "         )\n",
    "  print(\"out length:\", len(out))\n",
    "\n",
    "  print(\"logits.shape\", out[0].shape)\n",
    "  # print(\"out.past_key_values[0][0].shape:\", out.past_key_values[0][0].shape)\n",
    "  print(\"----out[0].shape: \", out[0].shape)\n",
    "  next_token_logits = out[0][:, -1, :]\n",
    "  next_tokens = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "  input_ids = tf.concat([input_ids, next_tokens[:, None]], axis=-1)\n",
    "  \n",
    "  print(\"next token predicted: \", next_tokens)\n",
    "  print(\"\\n\\n\")\n",
    "  # need to update past_key_values as apart of model_kwargs\n",
    "  # need to update size of mask (looks like we did, not sure about proper values)\n",
    "  # past_key_value shape[0][0]: ([1, 32, 11, 80]) after first run of model in orginal code\n",
    "  attention_mask = model_kwargs[\"attention_mask\"]\n",
    "  model_kwargs[\"attention_mask\"] = tf.concat([attention_mask, \n",
    "                                                 tf.ones((attention_mask.shape[0], 1), \n",
    "                                                         dtype=attention_mask.dtype)], \n",
    "                                                         axis=-1)\n",
    "  print(\"model_kwargs[attention_mask].shape\", model_kwargs[\"attention_mask\"].shape)\n",
    "  model_kwargs[\"past_key_values\"] = out[1]\n",
    "  # ending stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing an actual realistic loop\n",
    "# eager mode loop\n",
    "\n",
    "importlib.reload(my_phi)\n",
    "importlib.reload(my_phi.bert)\n",
    "importlib.reload(my_phi.masking_utils_gm)\n",
    "importlib.reload(my_phi.cache)\n",
    "my_model = None\n",
    "# config = PhiConfig(num_hidden_layers=3) # old line\n",
    "config = PhiConfig(use_cache=True)\n",
    "my_model = my_phi.PhiForCausalLM(config, params)\n",
    "\n",
    "input_ids = tf.constant([[2061, 318, 534, 4004, 3124, 30, 13, 11517, 318, 2266, 13]])\n",
    "mask = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "model_kwargs = {'use_cache': True, 'attention_mask': mask}\n",
    "\n",
    "# model_inputs(keys) after first call in orginal code: dict_keys(['input_ids', 'position_ids', 'past_key_values', 'use_cache', 'attention_mask'])\n",
    "\n",
    "# prepare_inputs_for_generation on first run looks good (not sure about 2nd)\n",
    "\n",
    "for i in range(1):\n",
    "    print(\"model_kwargs:\", model_kwargs)\n",
    "    model_inputs = my_model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "    print(\"---------\\nMODEL_INPUTS:\\n\", \"MASK:\" , model_inputs['attention_mask'])\n",
    "    print(\"POS_IDS:\", model_inputs['position_ids'], \"\\n\\n---------\\n\")\n",
    "    # print(\"model_inputs\", model_inputs)\n",
    "    # print(model_inputs.keys())\n",
    "    out = my_model(\n",
    "        **model_inputs,\n",
    "        return_dict=False, # important\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "        )\n",
    "    print(\"out length:\", len(out))\n",
    "    print(\"logits.shape\", out[0].shape)\n",
    "    # print(\"out.past_key_values[0][0].shape:\", out.past_key_values[0][0].shape)\n",
    "    print(\"----out[0].shape: \", out[0].shape)\n",
    "    next_token_logits = out[0][:, -1, :]\n",
    "    next_tokens = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "    input_ids = tf.concat([input_ids, next_tokens[:, None]], axis=-1)\n",
    "\n",
    "    print(\"next token predicted: \", next_tokens)\n",
    "    print(\"\\n\\n\")\n",
    "    # need to update past_key_values as apart of model_kwargs\n",
    "    # need to update size of mask (looks like we did, not sure about proper values)\n",
    "    # past_key_value shape[0][0]: ([1, 32, 11, 80]) after first run of model in orginal code\n",
    "    attention_mask = model_kwargs[\"attention_mask\"]\n",
    "    model_kwargs[\"attention_mask\"] = tf.concat([attention_mask, tf.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype)], axis=-1)\n",
    "    print(\"model_kwargs[attention_mask].shape\", model_kwargs[\"attention_mask\"].shape)\n",
    "    model_kwargs[\"past_key_values\"] = out[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@tf.function\n",
    "def test(input):\n",
    "    x = math.sqrt(3)\n",
    "    return tf.nn.softmax((1 - tf.experimental.numpy.triu(input, k=3)) / x)\n",
    "    # print(tf.math.reduce_max(input).shape)\n",
    "\n",
    "@tf.function\n",
    "def test2(input):\n",
    "    x1 = test(input)\n",
    "    print(x1)\n",
    "    return [[tf.cast(x1, dtype=tf.float16), x1], x1]\n",
    "\n",
    "@tf.function\n",
    "def test3(input):\n",
    "    return 1 + test2(input)[0][1]\n",
    "\n",
    "@tf.function\n",
    "def test4(i1, i2):\n",
    "    if i2 is None:\n",
    "        i2 = tf.ones([4,4])\n",
    "    i1 += 2\n",
    "    return i1, i2\n",
    "\n",
    "@tf.function\n",
    "def test5(i1, i2):\n",
    "    x1, x2 = test4(i1, i2)\n",
    "    x2 = [tf.ones([4,4]), tf.ones([4,4])]\n",
    "    x2[0] += 1\n",
    "    return test4(x1, x2)\n",
    "\n",
    "@tf.function\n",
    "def test6(i1):\n",
    "    i2 = [i[:] for i in i1] # allows us to create a deep copy of a list(list(tensors))\n",
    "    # i2 = i1[:]\n",
    "    # print(i2)\n",
    "    i2[1][0] += i2[1][0]\n",
    "    return i2\n",
    "\n",
    "x = [[tf.zeros([4,]),], [tf.ones([4,]),]]\n",
    "# x = [tf.zeros([4,]),]\n",
    "out = test6(x)\n",
    "print(out)\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
