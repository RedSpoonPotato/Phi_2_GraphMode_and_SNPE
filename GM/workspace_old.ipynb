{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:14:20.767886: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-28 21:14:21.345634: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-28 21:14:21.348179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 21:14:22.625046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:14:25.144704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-28 21:14:25.146500: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dtype = tf.float16 # subject to change\n",
    "BASE_DIR = \"../../phi_local/weights/\"\n",
    "DECODER_LAYERS = 4\n",
    "params = {}\n",
    "params['embed_tokens'] = tf.constant(np.load(BASE_DIR + 'embed_tokens.npy'), dtype=dtype)\n",
    "params['decoder_layers'] = []\n",
    "\n",
    "for i in range(DECODER_LAYERS):\n",
    "  layer_params = {}\n",
    "  layer_params['layernorm_weight'] = 'layernorm_weight.npy'\n",
    "  layer_params['layernorm_bias'] = 'layernorm_bias'\n",
    "  layer_params['q_proj_weight'] = 'q_proj_weight'\n",
    "  layer_params['q_proj_bias'] = 'q_proj_bias'\n",
    "  layer_params['k_proj_weight'] = 'k_proj_weight'\n",
    "  layer_params['k_proj_bias'] = 'k_proj_bias'\n",
    "  layer_params['v_proj_weight'] = 'v_proj_weight'\n",
    "  layer_params['v_proj_bias'] = 'v_proj_bias'\n",
    "  layer_params['dense_weight'] = 'dense_weight'\n",
    "  layer_params['dense_bias'] = 'dense_bias'\n",
    "  layer_params['mlp_fc1_weight'] = 'mlp_fc1_weight'\n",
    "  layer_params['mlp_fc1_bias'] = 'mlp_fc1_bias'\n",
    "  layer_params['mlp_fc2_weight'] = 'mlp_fc2_weight'\n",
    "  layer_params['mlp_fc2_bias'] = 'mlp_fc2_bias'\n",
    "  for key in layer_params:\n",
    "    layer_params[key] = tf.constant(np.load(BASE_DIR + str(i) + '_' + key + '.npy'), dtype=dtype)\n",
    "    if \"weight\" in key and \"layernorm\" not in key:\n",
    "      layer_params[key] = tf.transpose(layer_params[key], perm=[1,0])\n",
    "  params['decoder_layers'].append(layer_params)\n",
    "\n",
    "params['final_layernorm_weight'] = tf.constant(np.load(BASE_DIR + 'final_layernorm_weight.npy'), dtype=dtype)\n",
    "params['final_layernorm_bias'] = tf.constant(np.load(BASE_DIR + 'final_layernorm_bias.npy'), dtype=dtype)\n",
    "\n",
    "params['lm_head_weight'] = tf.transpose(tf.constant(np.load(BASE_DIR + 'lm_head_weight.npy'), dtype=dtype), perm=[1,0])\n",
    "params['lm_head_bias'] = tf.constant(np.load(BASE_DIR + 'lm_head_bias.npy'), dtype=dtype)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiConfig():\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=51200,\n",
    "        hidden_size=2560,\n",
    "        intermediate_size=10240,\n",
    "        num_hidden_layers=DECODER_LAYERS, # modified,\n",
    "        num_attention_heads=32,\n",
    "        num_key_value_heads=32,\n",
    "        resid_pdrop=0.0, # the model bt default has 0.1 on colab, but we dont have randomness\n",
    "        embd_pdrop=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        hidden_act=\"gelu_new\",\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.00,\n",
    "        layer_norm_eps=1e-5,\n",
    "        use_cache=True,                  # Modifed\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        partial_rotary_factor=0.4,\n",
    "        qk_layernorm=False,\n",
    "        bos_token_id=50256,\n",
    "        eos_token_id=50256,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        if num_key_value_heads is None:\n",
    "            num_key_value_heads = num_attention_heads\n",
    "\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.hidden_act = hidden_act\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.partial_rotary_factor = partial_rotary_factor\n",
    "        self.qk_layernorm = qk_layernorm\n",
    "        self.pad_token_id = None\n",
    "        self._attn_implementation = 'eager'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_phi_gm\n",
    "config_gm = PhiConfig()\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_key_values before for loop []\n",
      "past_key_values []\n",
      "past_key_value in decoder: []\n",
      "past key value_old in decoder layer: []\n",
      "past_key_value after tf.identity: []\n",
      "layer_id:  0\n",
      "past_key_value: []\n",
      "past_key_value after appending: [[<tf.Tensor 'concat_1:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'transpose_2:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "done updating\n",
      "next decoder cache in loop iteration: [[<tf.Tensor 'PartitionedCall_3:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_3:2' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_values []\n",
      "past_key_value in decoder: [[<tf.Tensor 'PartitionedCall_3:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_3:2' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past key value_old in decoder layer: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_value after tf.identity: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "layer_id:  1\n",
      "past_key_value: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_value after appending: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'concat_1:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'transpose_2:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "done updating\n",
      "next decoder cache in loop iteration: [[<tf.Tensor 'PartitionedCall_6:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_6:2' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_6:3' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_6:4' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_values []\n",
      "past_key_value in decoder: [[<tf.Tensor 'PartitionedCall_6:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_6:2' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_6:3' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_6:4' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past key value_old in decoder layer: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_value after tf.identity: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "layer_id:  2\n",
      "past_key_value: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_value after appending: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'concat_1:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'transpose_2:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "done updating\n",
      "next decoder cache in loop iteration: [[<tf.Tensor 'PartitionedCall_9:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_9:2' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_9:3' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_9:4' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_9:5' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_9:6' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_values []\n",
      "past_key_value in decoder: [[<tf.Tensor 'PartitionedCall_9:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_9:2' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_9:3' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_9:4' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_9:5' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_9:6' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past key value_old in decoder layer: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_4:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_5:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_value after tf.identity: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_4:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_5:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "layer_id:  3\n",
      "past_key_value: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_4:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_5:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "past_key_value after appending: [[<tf.Tensor 'past_key_value_old:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_1:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_2:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_3:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'past_key_value_old_4:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'past_key_value_old_5:0' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'concat_1:0' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'transpose_2:0' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "done updating\n",
      "next decoder cache in loop iteration: [[<tf.Tensor 'PartitionedCall_12:1' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_12:2' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_12:3' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_12:4' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_12:5' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_12:6' shape=(1, 32, 11, 80) dtype=float16>], [<tf.Tensor 'PartitionedCall_12:7' shape=(1, 32, 11, 80) dtype=float16>, <tf.Tensor 'PartitionedCall_12:8' shape=(1, 32, 11, 80) dtype=float16>]]\n",
      "tf.Tensor(\n",
      "[[[ 1.6738281   2.2460938   1.9160156  ... -0.45166016 -0.45166016\n",
      "   -0.45166016]\n",
      "  [ 2.5390625   1.8876953  -2.3183594  ... -0.19628906 -0.19628906\n",
      "   -0.19580078]\n",
      "  [ 1.9785156   2.25       -1.1855469  ... -0.6225586  -0.6230469\n",
      "   -0.62158203]\n",
      "  ...\n",
      "  [ 4.1953125   3.0625      1.015625   ... -0.5239258  -0.52197266\n",
      "   -0.52197266]\n",
      "  [ 3.4511719   0.7675781   0.24023438 ...  0.08624268  0.08685303\n",
      "    0.08636475]\n",
      "  [ 2.1347656  -1.9433594  -0.48779297 ...  0.3684082   0.36669922\n",
      "    0.36816406]]], shape=(1, 11, 51200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(my_phi_gm)\n",
    "importlib.reload(my_phi_gm.bert)\n",
    "importlib.reload(my_phi_gm.masking_utils_gm)\n",
    "importlib.reload(my_phi_gm.cache)\n",
    "config_gm = PhiConfig(use_cache=True)\n",
    "my_model_gm = None\n",
    "my_model_gm = my_phi_gm.PhiForCausalLM(config_gm, params)\n",
    "# testing input to see if graphmode works\n",
    "input_ids_gm = tf.constant([[40, 588, 836, 5500, 13, 1867, 318, 534, 4004, 2057, 30]])\n",
    "mask_gm = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "model_kwargs_gm = {'use_cache': True, 'attention_mask': mask_gm}\n",
    "model_inputs_gm = my_model_gm.prepare_inputs_for_generation(input_ids_gm, **model_kwargs_gm)\n",
    "out_gm = my_model_gm(\n",
    "        **model_inputs_gm,\n",
    "        return_dict=False, # important\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "        )\n",
    "print(out_gm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_ids: tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "hidden_states shape (1, 11, 2560)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'cache.DynamicCache'>\n",
      "position_ids: tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "hidden_states shape (1, 11, 2560)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Dense_v2.__call__ at 0x7f28155f4ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Dense_v2.__call__ at 0x7f281611e280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'cache.DynamicCache'>\n",
      "position_ids: tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "hidden_states shape (1, 11, 2560)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'cache.DynamicCache'>\n",
      "position_ids: tf.Tensor([[ 0  1  2  3  4  5  6  7  8  9 10]], shape=(1, 11), dtype=int32)\n",
      "mask: tf.Tensor(\n",
      "[[[[     0. -65504. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0. -65504. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0. -65504. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0. -65504. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0. -65504. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0. -65504. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0. -65504.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "    -65504. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0. -65504. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0. -65504.]\n",
      "   [     0.      0.      0.      0.      0.      0.      0.      0.\n",
      "         0.      0.      0.]]]], shape=(1, 1, 11, 11), dtype=float16)\n",
      "hidden_states shape (1, 11, 2560)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'cache.DynamicCache'>\n",
      "tf.Tensor(\n",
      "[[[ 1.6738281   2.2460938   1.9160156  ... -0.45166016 -0.45166016\n",
      "   -0.45166016]\n",
      "  [ 2.5429688   1.8837891  -2.3085938  ... -0.19824219 -0.19702148\n",
      "   -0.19824219]\n",
      "  [ 1.9902344   2.2578125  -1.1855469  ... -0.6245117  -0.6254883\n",
      "   -0.62353516]\n",
      "  ...\n",
      "  [ 4.203125    3.0644531   1.0136719  ... -0.52001953 -0.51953125\n",
      "   -0.5185547 ]\n",
      "  [ 3.4492188   0.76953125  0.2319336  ...  0.08575439  0.08551025\n",
      "    0.08575439]\n",
      "  [ 2.1289062  -1.9453125  -0.4975586  ...  0.36645508  0.36767578\n",
      "    0.36743164]]], shape=(1, 11, 51200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# testing input to see if graphmode works\n",
    "\n",
    "import my_phi\n",
    "importlib.reload(my_phi)\n",
    "importlib.reload(my_phi.bert)\n",
    "importlib.reload(my_phi.masking_utils)\n",
    "importlib.reload(my_phi.cache)\n",
    "my_model = None\n",
    "my_model = my_phi.PhiForCausalLM(config, params)\n",
    "input_ids = tf.constant([[40, 588, 836, 5500, 13, 1867, 318, 534, 4004, 2057, 30]])\n",
    "mask = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "model_kwargs = {'use_cache': True, 'attention_mask': mask}\n",
    "model_inputs = my_model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "out = my_model(\n",
    "        **model_inputs,\n",
    "        return_dict=True,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "        )\n",
    "print(out.logits)\n",
    "# testing an self-attntion-layer\n",
    "# input = tf.random.uniform([1, 11, 2560], dtype=tf.float16)\n",
    "# attn = None\n",
    "# pos_ids = tf.range([11])\n",
    "# print(pos_ids)\n",
    "# attn = my_phi.PhiAttention(config, params['decoder_layers'][0], 0)\n",
    "# attn(input, pos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>], [<tf.Tensor: shape=(4,), dtype=float32, numpy=array([2., 2., 2., 2.], dtype=float32)>]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "@tf.function\n",
    "def test(input):\n",
    "    x = math.sqrt(3)\n",
    "    return tf.nn.softmax((1 - tf.experimental.numpy.triu(input, k=3)) / x)\n",
    "    # print(tf.math.reduce_max(input).shape)\n",
    "\n",
    "@tf.function\n",
    "def test2(input):\n",
    "    x1 = test(input)\n",
    "    print(x1)\n",
    "    return [[tf.cast(x1, dtype=tf.float16), x1], x1]\n",
    "\n",
    "@tf.function\n",
    "def test3(input):\n",
    "    return 1 + test2(input)[0][1]\n",
    "\n",
    "@tf.function\n",
    "def test4(i1, i2):\n",
    "    if i2 is None:\n",
    "        i2 = tf.ones([4,4])\n",
    "    i1 += 2\n",
    "    return i1, i2\n",
    "\n",
    "@tf.function\n",
    "def test5(i1, i2):\n",
    "    x1, x2 = test4(i1, i2)\n",
    "    x2 = [tf.ones([4,4]), tf.ones([4,4])]\n",
    "    x2[0] += 1\n",
    "    return test4(x1, x2)\n",
    "\n",
    "@tf.function\n",
    "def test6(i1):\n",
    "    i2 = [i[:] for i in i1] # allows us to create a deep copy of a list(list(tensors))\n",
    "    # i2 = i1[:]\n",
    "    # print(i2)\n",
    "    i2[1][0] += i2[1][0]\n",
    "    return i2\n",
    "\n",
    "x = [[tf.zeros([4,]),], [tf.ones([4,]),]]\n",
    "# x = [tf.zeros([4,]),]\n",
    "out = test6(x)\n",
    "print(out)\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
